{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd345876-2548-4b45-82f6-905d22b4b7fc",
   "metadata": {},
   "source": [
    "# Qualitative Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f476cd2-9aa0-4fba-afdd-7528baaf7320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from kneed import KneeLocator\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "from textblob import TextBlob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import pipeline\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bedcc32-9628-466b-b186-768ac15e05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_map = pd.read_csv(\"qual_map.csv\")\n",
    "\n",
    "qual_map_loaded = {key: group.drop(columns=[\"key\"]) for key, group in qual_map.groupby(\"key\")}\n",
    "\n",
    "qual_map = qual_map_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb9e81f-9b70-4712-9ac5-3576cd8ed583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qual_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c7850-bca5-442c-97a4-6a2e77bd4d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_labels = ['Playing Ability','Competitive', 'Character', 'Team Player', 'Leadership', 'Passion', \n",
    "                    'Body Language', 'Selfless', 'Low Ego', 'Loyalty', 'Teamwork', 'Trustworthy', 'Dependable', 'Integrity', 'Honest',\n",
    "                   'Maturity', 'Responsible', 'Positive', 'Confident', 'Adaptive', 'Work Ethic', 'Driven', 'Resilient', 'Effort']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0fb73-b24b-45ce-aa4f-2e06f790f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunctions = [\n",
    "    'for', 'and', 'nor', 'but', 'or', 'yet', 'so',\n",
    "    'because', 'since', 'as', 'seeing that', 'inasmuch as', 'now that', 'considering that',\n",
    "    'when', 'whenever', 'while', 'after', 'before', 'until', 'as soon as', 'as long as', 'once',\n",
    "    'if', 'unless', 'provided that', 'assuming that', 'in case', 'even if', 'supposing that',\n",
    "    'although', 'though', 'even though', 'whereas', 'despite the fact that',\n",
    "    'so that', 'in order that', 'lest',\n",
    "    'just as', 'as if', 'as though', 'than',\n",
    "    'such that',\n",
    "    'either', 'or', 'neither', 'nor', 'both', 'and', 'not only', 'but also', 'whether',\n",
    "    'just as', 'so', 'as much', 'as', 'no sooner', 'than', 'rather', 'the more', 'the more',\n",
    "    'moreover', 'furthermore', 'besides', 'in addition', 'not to mention',\n",
    "    'however', 'on the other hand', 'nevertheless', 'nonetheless', 'conversely',\n",
    "    'therefore', 'consequently', 'thus', 'as a result', 'hence',\n",
    "    'then', 'thereafter', 'subsequently', 'meanwhile', 'at the same time',\n",
    "    'otherwise',\n",
    "    'likewise', 'similarly'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d31999-8489-4d43-ba96-f0e917672410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_scouting_analysis(nfl_data, candidate_labels, conjunctions):\n",
    "    \n",
    "    for pos, df in nfl_data.items():\n",
    "        \n",
    "        if 'scouting' in df.columns:\n",
    "            print(f'Applying sentiment analysis for position: {pos} (Data= {len(df)})')\n",
    "\n",
    "            df['scouting'] = df['scouting'].fillna('').astype(str)\n",
    "\n",
    "            df_results = df['scouting'].apply(lambda text: analyze_scouting_report(text, candidate_labels, conjunctions))\n",
    "\n",
    "            df_expanded = df_results.apply(pd.Series)\n",
    "\n",
    "            expected_categories = candidate_labels\n",
    "            for col in expected_categories:\n",
    "                if col not in df_expanded.columns:\n",
    "                    df_expanded[col] = 0\n",
    "                    \n",
    "            df.drop(columns=['scouting'], inplace=True)\n",
    "\n",
    "            nfl_data[pos] = pd.concat([df, df_expanded], axis=1)\n",
    "\n",
    "\n",
    "    return nfl_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc1a62-8302-40b2-892e-2b98589acbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "player_id_mapping = pd.read_csv(\"player_id_mapping.csv\")\n",
    "\n",
    "classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "roberta_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "distilbert_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf0df03-2175-47e4-99da-0133f2b1a601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_scouting_report(scouting_report, candidate_labels, conjunctions):\n",
    "\n",
    "    if not isinstance(scouting_report, str) or scouting_report.strip() == \"\":\n",
    "        return {label: 0 for label in candidate_labels}\n",
    "\n",
    "    sentences = [s.strip() for s in scouting_report.split('.') if s.strip()] \n",
    "\n",
    "    def split_by_conjunctions(sentence, conjunctions):\n",
    "        for conj in conjunctions:\n",
    "            if conj in sentence:\n",
    "                return [part.strip() for part in sentence.split(conj) if part.strip()]\n",
    "        return [sentence.strip()]\n",
    "\n",
    "    relevance_scores = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        fragments = split_by_conjunctions(sentence, conjunctions)\n",
    "\n",
    "        fragments = [frag for frag in fragments if len(frag.split()) >= 3]\n",
    "\n",
    "        for fragment in fragments:\n",
    "            try:\n",
    "                result = classifier(fragment, candidate_labels)\n",
    "                relevance_scores[fragment] = dict(zip(result['labels'], result['scores']))\n",
    "            except ValueError as e:\n",
    "                print('Relevance score issue') \n",
    "\n",
    "    grouped_fragments = {label: [] for label in candidate_labels}\n",
    "    threshold = 0.2\n",
    "\n",
    "    for fragment, scores in relevance_scores.items():\n",
    "        for label, score in scores.items():\n",
    "            if score > threshold:\n",
    "                grouped_fragments[label].append(fragment.strip())\n",
    "\n",
    "    sentiment_results = {}\n",
    "\n",
    "    for category, fragments in grouped_fragments.items():\n",
    "        sentiment_scores = []\n",
    "\n",
    "        for fragment in fragments:\n",
    "            try:\n",
    "                roberta_sentiment = roberta_analyzer(fragment)\n",
    "                roberta_confidence = roberta_sentiment[0]['score']\n",
    "\n",
    "                distilbert_sentiment = distilbert_analyzer(fragment)\n",
    "                distilbert_label = distilbert_sentiment[0]['label']\n",
    "                distilbert_score = distilbert_sentiment[0]['score']\n",
    "\n",
    "                if distilbert_label == 'POSITIVE':\n",
    "                    combined_score = distilbert_score * roberta_confidence\n",
    "                elif distilbert_label == 'NEGATIVE':\n",
    "                    combined_score = -distilbert_score * roberta_confidence\n",
    "                else:\n",
    "                    combined_score = 0\n",
    "\n",
    "                sentiment_scores.append(combined_score)\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Issue with sentiment analysis')\n",
    "\n",
    "        sentiment_results[category] = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0\n",
    "\n",
    "    return sentiment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b53f72-4b7b-4018-bf73-e2a651d9891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qual_map = apply_scouting_analysis(qual_map, candidate_labels, conjunctions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8590f6-edd7-4c9c-976b-61b38da158e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for position, df in qual_map.items():\n",
    "    if all(label in df.columns for label in candidate_labels):\n",
    "        scaler = MinMaxScaler()\n",
    "        df[candidate_labels] = scaler.fit_transform(df[candidate_labels])\n",
    "    \n",
    "    qual_map[position] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85500762-f7ba-4104-bad1-8c01f138922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(qual_map['QB'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48825b47-4d11-4b27-bf82-66df7d02bbf5",
   "metadata": {},
   "source": [
    "# Open Competiton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf240d-0667-4678-9ae4-22a3ad8c9ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_important_features(lib, candidate_labels, top_n=3, random_seed=42):\n",
    "    np.random.seed(random_seed)\n",
    "    feature_selected_lib = {}\n",
    "\n",
    "    for pos, value in lib.items():\n",
    "        df = value['DataFrame']\n",
    "\n",
    "        player_ids = df['player_id']\n",
    "\n",
    "        df_no_id = df.drop(columns=['player_id', 'cluster'], errors='ignore')\n",
    "\n",
    "        random_labels = np.random.randint(0, 2, size=len(df_no_id))\n",
    "\n",
    "        # Random Forest Classifier for identifying important features\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=random_seed)\n",
    "        rf.fit(df_no_id, random_labels)\n",
    "        feature_importances = rf.feature_importances_\n",
    "\n",
    "        importance_df = pd.DataFrame({'Feature': df_no_id.columns, 'Importance': feature_importances})\n",
    "        importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "        selected_features = importance_df['Feature'][:top_n].tolist()\n",
    "\n",
    "        df_selected = df[selected_features].copy()\n",
    "        df_selected['player_id'] = player_ids\n",
    "\n",
    "        feature_selected_lib[pos] = {'DataFrame': df_selected, 'Optimal_k': value['Optimal_k']}\n",
    "\n",
    "        print(f\"Position: {pos} - Selected Features: {selected_features}\")\n",
    "\n",
    "    return feature_selected_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310dd84d-0917-411b-85b4-b52d388a1a92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_elbow(pos_mapping):\n",
    "    def elbow(df, name, ax):\n",
    "        \n",
    "        player_ids = df[\"player_id\"]\n",
    "\n",
    "        df_no_id = df.drop(columns=[\"player_id\"])\n",
    "        \n",
    "        inertias = []\n",
    "        cluster_range = range(2, min(len(df_no_id), 15))\n",
    "        \n",
    "        for k in cluster_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(df_no_id)\n",
    "            inertias.append(kmeans.inertia_)\n",
    "    \n",
    "        knee_locator = KneeLocator(cluster_range, inertias, curve=\"convex\", direction=\"decreasing\")\n",
    "        optimal_k = knee_locator.knee\n",
    "\n",
    "        # If no optimal k set to 3\n",
    "        if optimal_k is None:\n",
    "            optimal_k = 3\n",
    "\n",
    "        ax.plot(cluster_range, inertias, marker='o')\n",
    "        ax.axvline(x=optimal_k, color=\"r\", linestyle=\"--\", label=f\"Optimal k={optimal_k}\")\n",
    "        ax.set_title(f'Elbow Method {name}')\n",
    "        ax.set_xlabel('Number of Clusters')\n",
    "        ax.set_ylabel('Inertia')\n",
    "        ax.legend()\n",
    "\n",
    "        return optimal_k, player_ids\n",
    "\n",
    "    qualitative = {}\n",
    "    \n",
    "    num_positions = len(pos_mapping)\n",
    "    rows = (num_positions // 3) + 1\n",
    "    cols = 3\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (pos, df) in enumerate(pos_mapping.items()):\n",
    "        opt_k, player_ids = elbow(df, pos, axes[i])\n",
    "\n",
    "        df['player_id'] = player_ids\n",
    "        qualitative[pos] = {'DataFrame': df, 'Optimal_k': opt_k}\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return qualitative\n",
    "\n",
    "qualitative = get_elbow(qual_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14d0bc-b045-4f86-a429-2ebd67850831",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_qualitative = select_important_features(qualitative, candidate_labels, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934814b1-c6f0-4517-a394-bb08da01e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering(lib):\n",
    "    \n",
    "    def evaluate_kmeans(params, data):\n",
    "        model = KMeans(**params)\n",
    "        labels = model.fit_predict(data)\n",
    "        score = silhouette_score(data, labels)\n",
    "        return score\n",
    "\n",
    "    for pos, value in lib.items():\n",
    "        df = value['DataFrame']\n",
    "        k = value['Optimal_k']\n",
    "\n",
    "        player_ids = df['player_id']\n",
    "        df_no_id = df.drop(columns=['player_id'])  \n",
    "\n",
    "        param_grid = {\n",
    "            'n_clusters': [k],\n",
    "            'init': ['k-means++', 'random'],\n",
    "            'max_iter': [50, 100, 300], \n",
    "            'random_state': [42]\n",
    "        }\n",
    "\n",
    "        param_grid = ParameterGrid(param_grid)\n",
    "        best_params = None\n",
    "        best_score = -1\n",
    "\n",
    "        # GridSearch to find optimal parameters\n",
    "        for params in param_grid:\n",
    "            score = evaluate_kmeans(params, df_no_id)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_params = params\n",
    "\n",
    "        optimal_kmeans = KMeans(**best_params)\n",
    "        cluster_labels = optimal_kmeans.fit_predict(df_no_id)\n",
    "\n",
    "        df['cluster'] = cluster_labels.astype(str)\n",
    "        df['player_id'] = player_ids  \n",
    "\n",
    "        lib[pos]['DataFrame'] = df\n",
    "\n",
    "    return {pos: info['DataFrame'] for pos, info in lib.items()}\n",
    "\n",
    "qual_cluster= clustering(filtered_qualitative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1eabe2-7ea7-49e1-910f-6751981eac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters_with_table(qualitative, player_id_mapping):\n",
    "    \n",
    "    for pos, df in qualitative.items():\n",
    "        df = df.reset_index()\n",
    "        df = df.merge(player_id_mapping, on='player_id', how='left')\n",
    "\n",
    "        df['cluster'] = df['cluster'].astype(int)\n",
    "        df['cluster'] = pd.Categorical(df['cluster'], categories=sorted(df['cluster'].unique()), ordered=True)\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_features = pca.fit_transform(\n",
    "            df.drop(columns=['cluster', 'player_name', 'pos_abbr', 'player_id'], errors='ignore')\n",
    "        )\n",
    "\n",
    "        # Add PCA features back to the DataFrame\n",
    "        df['PCA1'] = pca_features[:, 0]\n",
    "        df['PCA2'] = pca_features[:, 1]\n",
    "\n",
    "        sorted_clusters = sorted(df['cluster'].unique())\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        scatter_plot = sns.scatterplot(\n",
    "            data=df,\n",
    "            x='PCA1',\n",
    "            y='PCA2',\n",
    "            hue='cluster',\n",
    "            palette='viridis',\n",
    "            s=100,\n",
    "            alpha=0.7,\n",
    "            hue_order=sorted_clusters\n",
    "        )\n",
    "        plt.title(f'Cluster Visualization of {pos} (PCA)', fontsize=16)\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "        cluster_info = []\n",
    "        grouped = df.groupby('cluster', observed=True)\n",
    "        for cluster, group in grouped:\n",
    "            cluster_text = [f'Cluster {cluster}\"] + [f\"{row['player_name']} ({row['pos_abbr']})' for _, row in group.iterrows()]\n",
    "            cluster_info.append(cluster_text)\n",
    "\n",
    "        max_rows_per_column = 20\n",
    "        flattened_table = []\n",
    "        for cluster_text in cluster_info:\n",
    "            flattened_table.extend(cluster_text)\n",
    "            flattened_table.append('')\n",
    "\n",
    "        num_columns = math.ceil(len(flattened_table) / max_rows_per_column)\n",
    "        table_data = [\n",
    "            flattened_table[i * max_rows_per_column:(i + 1) * max_rows_per_column]\n",
    "            for i in range(num_columns)\n",
    "        ]\n",
    "\n",
    "        max_col_length = max(len(column) for column in table_data)\n",
    "        table_data = [\n",
    "            column + [''] * (max_col_length - len(column)) for column in table_data\n",
    "        ]\n",
    "\n",
    "        table_ax = plt.gcf().add_axes([0.1, -0.4, 0.8, 0.3])\n",
    "        table_ax.axis('off')\n",
    "        table = table_ax.table(\n",
    "            cellText=list(zip(*table_data)),\n",
    "            cellLoc='left',\n",
    "            loc='center',\n",
    "        )\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(10)\n",
    "        table.auto_set_column_width(col=list(range(len(table_data))))\n",
    "\n",
    "        # Show the plot\n",
    "        plt.subplots_adjust(bottom=0.12)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cda971f-76ee-497f-b605-3479debeaf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters_with_table(qual_cluster, player_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26590092-e4cd-4fc2-b38c-fe4c686a3f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos, df in qual_cluster.items():\n",
    "    qual_cluster[pos] = qual_cluster[pos].merge(player_id_mapping, on='player_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc948be-f911-4376-9cad-c6dfa559c00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('qual_assignments.pkl', 'wb') as f:\n",
    "    pickle.dump(qual_cluster, f)\n",
    "\n",
    "\n",
    "\n",
    "qual_cluster_df = pd.concat([df.assign(position=pos) for pos, df in qual_cluster.items()])\n",
    "quant_cluster_df.to_csv('qual_assignments_list.csv', index = False)\n",
    "\n",
    "\n",
    "\n",
    "qual_cluster_df = pd.concat([df.assign(position=pos) for pos, df in qual_cluster.items()])\n",
    "qual_cluster_df = qual_cluster_df[['player_id', 'cluster', 'position', 'player_name', 'draft_year']]\n",
    "\n",
    "# Save to CSV\n",
    "qual_cluster_df.to_csv('qual_assignments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9c3414-c06c-472f-afe4-8979909cbabe",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b472b9d7-510f-47b9-adc2-3915bea6f1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
